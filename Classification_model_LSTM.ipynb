{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chemical-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader , random_split\n",
    "from torchvision import utils, transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "special-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Deepfake_Dataloader import DeepfakeFaceDataset, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "encouraging-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 1024\n",
    "HIDDEN_DIM = 1024\n",
    "LAYERS = 2\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "closing-harvey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "num_trainset = 1080\n",
    "csv_path = 'metadata_sample3.csv'\n",
    "file_path = 'sample/'\n",
    "\n",
    "transformed_dataset = DeepfakeFaceDataset(csv_file=csv_path,\n",
    "                                        root_dir=file_path,\n",
    "                                        transform=transforms.Compose([\n",
    "                                                ToTensor()\n",
    "                                           ]))\n",
    "train_set, val_set = random_split(transformed_dataset, [num_trainset, len(transformed_dataset)-num_trainset])\n",
    "print(len(transformed_dataset)-num_trainset)\n",
    "trn_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "packed-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 항상 torch.nn.Module을 상속받고 시작\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        conv1 = nn.Conv2d(3, 8, (3,3), 1)\n",
    "        pool1 = nn.MaxPool2d(2)\n",
    "        conv2 = nn.Conv2d(8, 8, (5,5), 1)\n",
    "        pool2 = nn.MaxPool2d(2)\n",
    "        conv3 = nn.Conv2d(8, 16, (5,5), 1)\n",
    "        pool3 = nn.MaxPool2d(2)\n",
    "        conv4 = nn.Conv2d(16, 16, (5,5), 1)\n",
    "        pool4 = nn.MaxPool2d(4)\n",
    "        \n",
    "        self.conv_module = nn.Sequential(\n",
    "            conv1,\n",
    "            nn.ReLU(),\n",
    "            pool1,\n",
    "            conv2,\n",
    "            nn.ReLU(),\n",
    "            pool2,\n",
    "            conv3,\n",
    "            nn.ReLU(),\n",
    "            pool3,\n",
    "            conv4,\n",
    "            nn.ReLU(),\n",
    "            pool4\n",
    "        )\n",
    "        \n",
    "        fc1 = nn.Linear(576, 1024)\n",
    "        dp1 = nn.Dropout(0.5)\n",
    "        fc2 = nn.Linear(1024, 16)\n",
    "        d2 = nn.Dropout(0.5)\n",
    "        fc3 = nn.Linear(16, 1024)\n",
    "\n",
    "        self.fc_module = nn.Sequential(\n",
    "            fc1,\n",
    "            dp1,\n",
    "            fc2,\n",
    "            nn.LeakyReLU(),\n",
    "            d2,\n",
    "            fc3\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_module(x)\n",
    "        dim = 1\n",
    "        for d in out.size()[1:]:\n",
    "            dim = dim * d\n",
    "        out = out.view(-1, dim)\n",
    "        out = self.fc_module(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sonic-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_variable_input(torch.nn.Module) :\n",
    "    def __init__(self, INPUT_DIM, HIDDEN_DIM, N_LAYERS):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.lstm = nn.LSTM(input_size = INPUT_DIM, hidden_size = HIDDEN_DIM, num_layers = N_LAYERS, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        #x = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
    "        out_pack, (ht, ct) = self.lstm(x)\n",
    "        print(out_pack)\n",
    "        print(ht.shape)\n",
    "        out = self.linear(out_pack[-1])\n",
    "        print(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "consistent-investigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (conv_module): Sequential(\n",
      "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_module): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=16, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM_variable_input(\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(1024, 1024, num_layers=2, batch_first=True)\n",
       "  (linear): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding_Model = CNNClassifier().double()\n",
    "Embedding_Model.load_state_dict(torch.load('c:/Users/jungw/Desktop/new/GAN/dfdc_pract/deepfake_database/model_2nd.pt'))\n",
    "print(Embedding_Model)\n",
    "\n",
    "model = LSTM_variable_input(1024, 1024, 2).double()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thick-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1,3,1024])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indonesian-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2]],\n",
      "\n",
      "        [[3, 4]]]) torch.Size([2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]])\n",
    "z=x.unsqueeze(1)\n",
    "print(z,z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cultural-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024])\n",
      "tensor([[[-0.0262,  0.0846, -0.0062,  ..., -0.0025,  0.1214, -0.0187],\n",
      "         [ 0.0157, -0.0118,  0.0374,  ...,  0.0055,  0.0373,  0.0013],\n",
      "         [ 0.0507, -0.0170,  0.0265,  ...,  0.0228,  0.0298, -0.0215]]],\n",
      "       dtype=torch.float64, grad_fn=<TransposeBackward0>)\n",
      "torch.Size([2, 1, 1024])\n",
      "tensor([[-0.0173],\n",
      "        [-0.0652],\n",
      "        [-0.0846]], dtype=torch.float64, grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([3, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d8322d6e6d1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\CodingPro\\Anaconda3\\envs\\pytorchcpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\CodingPro\\Anaconda3\\envs\\pytorchcpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\CodingPro\\Anaconda3\\envs\\pytorchcpu\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2517\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2518\u001b[0m         raise ValueError(\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\n\u001b[1;32m-> 2519\u001b[1;33m                          \"Please ensure they have the same size.\".format(target.size(), input.size()))\n\u001b[0m\u001b[0;32m   2520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2521\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([3, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_batches = len(trn_loader)\n",
    "use_cuda = False\n",
    "\n",
    "model.train()\n",
    "\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    trn_loss = 0.0\n",
    "    for i,data in enumerate(trn_loader):\n",
    "        x = torch.DoubleTensor(data['image']/255)\n",
    "        label = data['label']\n",
    "        if use_cuda:\n",
    "            x = torch.cuda.DoubleTensor(x.cuda())\n",
    "            label = label.cuda()\n",
    "        \n",
    "        inputs = Embedding_Model(x.squeeze(0))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        print(inputs.unsqueeze(0).shape)\n",
    "        output = model(inputs.unsqueeze(0))        \n",
    "        loss = criterion(output, ((label+1)/2).type(torch.DoubleTensor))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "        \n",
    "        if (i+1) % 100 == 0: # every 100 mini-batches\n",
    "            with torch.no_grad(): # very very very very important!!!\n",
    "                val_loss = 0.0\n",
    "                for j, val in enumerate(val_loader):\n",
    "                    \n",
    "                    val_x = torch.DoubleTensor(data['image']/255)\n",
    "                    val_label = data['label']\n",
    "                    if use_cuda:\n",
    "                        val_x = torch.cuda.DoubleTensor(val_x.cuda())\n",
    "                        val_label = val_label.cuda()\n",
    "\n",
    "                    inputs_val = Embedding_Model(val_x.squeeze(0))\n",
    "                    \n",
    "                    # forward propagation\n",
    "                    output = model(inputs_val.unsqueeze(0))\n",
    "                    v_loss = criterion(output, ((val_label+1)/2).type(torch.DoubleTensor))\n",
    "                    \n",
    "                    val_loss += v_loss\n",
    "                    \n",
    "            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.8f} | val loss: {:.8f}\".format(\n",
    "                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(val_loader)\n",
    "            ))\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': trn_loss/100\n",
    "            }, 'c:/Users/jungw/Desktop/new/GAN/dfdc_pract/deepfake_database/rnn_model_1st.pt')\n",
    "            \n",
    "            trn_loss_list.append(trn_loss/100)\n",
    "            val_loss_list.append(val_loss/len(val_loader))\n",
    "            trn_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-survey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
